name: Performance Testing

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
    types: [opened, synchronize, reopened]
  schedule:
    # Run weekly performance checks
    - cron: '0 6 * * 1'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test'
        required: true
        default: 'lighthouse'
        type: choice
        options:
          - lighthouse
          - load
          - comprehensive

# Prevent concurrent performance tests
concurrency:
  group: performance-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_VERSION: '18'
  PERFORMANCE_BUDGET_CPU: '85'  # Performance budget for CPU score
  PERFORMANCE_BUDGET_MEMORY: '90'  # Performance budget for memory score
  LIGHTHOUSE_MIN_SCORE: '85'  # Minimum acceptable Lighthouse score

jobs:
  # Lighthouse Performance Testing
  lighthouse-audit:
    name: ğŸ” Lighthouse Performance Audit
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.test_type == 'lighthouse' || github.event.inputs.test_type == 'comprehensive' || github.event.inputs.test_type == ''

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ“¦ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ğŸ“¦ Install dependencies
        run: npm ci

      - name: ğŸ—ï¸ Build application
        run: npm run build
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}

      - name: ğŸš€ Start production server
        run: npm start &
        env:
          PORT: 3000
          NODE_ENV: production

      - name: â³ Wait for server
        run: npx wait-on http://localhost:3000 --timeout 60000

      - name: ğŸ” Run Lighthouse CI
        run: npx lighthouse-ci autorun
        env:
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}

      - name: ğŸ“Š Upload Lighthouse reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-reports-${{ github.run_number }}
          path: |
            .lighthouseci/
            reports/
          retention-days: 14

      - name: ğŸš¨ Check performance budgets
        run: |
          # Parse Lighthouse results and check budgets
          if [ -f ".lighthouseci/manifest.json" ]; then
            echo "Checking Lighthouse performance budgets..."
            
            # Extract scores from manifest (simplified check)
            PERF_SCORE=$(cat .lighthouseci/manifest.json | jq -r '.[0].summary.performance // 0' | cut -d. -f1)
            ACCESSIBILITY_SCORE=$(cat .lighthouseci/manifest.json | jq -r '.[0].summary.accessibility // 0' | cut -d. -f1)
            
            echo "Performance Score: $PERF_SCORE"
            echo "Accessibility Score: $ACCESSIBILITY_SCORE"
            
            if [ "$PERF_SCORE" -lt "${{ env.LIGHTHOUSE_MIN_SCORE }}" ]; then
              echo "âŒ Performance score ($PERF_SCORE) is below budget (${{ env.LIGHTHOUSE_MIN_SCORE }})"
              exit 1
            fi
            
            if [ "$ACCESSIBILITY_SCORE" -lt "90" ]; then
              echo "âš ï¸ Accessibility score ($ACCESSIBILITY_SCORE) is below 90"
            fi
            
            echo "âœ… Performance budgets met"
          else
            echo "âš ï¸ No Lighthouse manifest found, skipping budget check"
          fi

  # Load Testing with Artillery
  load-testing:
    name: âš¡ Load Testing
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event.inputs.test_type == 'load' || github.event.inputs.test_type == 'comprehensive' || (github.event_name == 'schedule')

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ“¦ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ğŸ“¦ Install dependencies
        run: npm ci

      - name: ğŸ—ï¸ Build application
        run: npm run build
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}

      - name: ğŸš€ Start production server
        run: npm start &
        env:
          PORT: 3000
          NODE_ENV: production

      - name: â³ Wait for server
        run: npx wait-on http://localhost:3000 --timeout 60000

      - name: âš¡ Run load tests
        run: npm run test:load
        continue-on-error: true

      - name: ğŸ“Š Upload load test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-reports-${{ github.run_number }}
          path: |
            tests/load/results/
            reports/performance/
          retention-days: 14

      - name: ğŸš¨ Analyze load test results
        run: |
          echo "Analyzing load test results..."
          
          if [ -f "tests/load/results/load-test-report.json" ]; then
            # Parse load test results
            AVG_RESPONSE_TIME=$(cat tests/load/results/load-test-report.json | jq -r '.aggregate.latency.mean // 0')
            P95_RESPONSE_TIME=$(cat tests/load/results/load-test-report.json | jq -r '.aggregate.latency.p95 // 0')
            ERROR_RATE=$(cat tests/load/results/load-test-report.json | jq -r '.aggregate.counters.errors // 0')
            
            echo "Average Response Time: ${AVG_RESPONSE_TIME}ms"
            echo "P95 Response Time: ${P95_RESPONSE_TIME}ms"
            echo "Error Rate: $ERROR_RATE"
            
            # Check thresholds
            if (( $(echo "$AVG_RESPONSE_TIME > 2000" | bc -l) )); then
              echo "âŒ Average response time exceeds 2000ms"
              exit 1
            fi
            
            if (( $(echo "$P95_RESPONSE_TIME > 5000" | bc -l) )); then
              echo "âŒ P95 response time exceeds 5000ms"
              exit 1
            fi
            
            if (( $(echo "$ERROR_RATE > 5" | bc -l) )); then
              echo "âŒ Error rate exceeds 5%"
              exit 1
            fi
            
            echo "âœ… Load test thresholds met"
          else
            echo "âš ï¸ Load test results not found"
          fi

  # Memory and CPU profiling
  resource-profiling:
    name: ğŸ§  Resource Profiling
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.test_type == 'comprehensive' || github.event_name == 'schedule'

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ“¦ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ğŸ“¦ Install dependencies
        run: npm ci

      - name: ğŸ§  Install profiling tools
        run: |
          npm install -g clinic
          npm install -g autocannon

      - name: ğŸ—ï¸ Build application
        run: npm run build
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}

      - name: ğŸ§  Profile application startup
        run: |
          echo "Starting memory and CPU profiling..."
          
          # Start the app with profiling
          timeout 120s clinic doctor --on-port 'autocannon -c10 -d30 http://localhost:3000' -- npm start || true
          
          # Generate flame graphs if available
          if [ -f ".clinic/doctor.html" ]; then
            echo "âœ… Profiling report generated"
          fi

      - name: ğŸ“Š Upload profiling reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: profiling-reports-${{ github.run_number }}
          path: |
            .clinic/
            reports/profiling/
          retention-days: 7

  # Bundle analysis
  bundle-analysis:
    name: ğŸ“¦ Bundle Size Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ“¦ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ğŸ“¦ Install dependencies
        run: npm ci

      - name: ğŸ—ï¸ Build with bundle analysis
        run: npm run build:analyze
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          ANALYZE: true

      - name: ğŸ“Š Check bundle size budgets
        run: |
          echo "Checking bundle size budgets..."
          
          # Check if bundle size report exists
          if [ -f ".next/analyze/bundle-sizes.json" ]; then
            TOTAL_SIZE=$(cat .next/analyze/bundle-sizes.json | jq -r '.sizes.total // 0')
            JS_SIZE=$(cat .next/analyze/bundle-sizes.json | jq -r '.sizes.javascript // 0')
            
            echo "Total Bundle Size: ${TOTAL_SIZE}KB"
            echo "JavaScript Size: ${JS_SIZE}KB"
            
            # Budget checks (in KB)
            if [ "$TOTAL_SIZE" -gt 1000 ]; then
              echo "âš ï¸ Total bundle size (${TOTAL_SIZE}KB) exceeds budget (1000KB)"
            fi
            
            if [ "$JS_SIZE" -gt 500 ]; then
              echo "âš ï¸ JavaScript bundle size (${JS_SIZE}KB) exceeds budget (500KB)"
            fi
            
            echo "âœ… Bundle analysis complete"
          else
            echo "ğŸ“Š Running basic bundle size check..."
            find .next -name "*.js" -type f -exec du -ch {} + | grep total
          fi

      - name: ğŸ“Š Upload bundle analysis
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bundle-analysis-${{ github.run_number }}
          path: |
            .next/analyze/
            reports/bundle/
          retention-days: 14

  # Performance summary
  performance-summary:
    name: ğŸ“Š Performance Summary
    runs-on: ubuntu-latest
    if: always()
    needs: [lighthouse-audit, load-testing, bundle-analysis]

    steps:
      - name: ğŸ“Š Create performance summary
        uses: actions/github-script@v7
        with:
          script: |
            const results = {
              'Lighthouse Audit': '${{ needs.lighthouse-audit.result }}',
              'Load Testing': '${{ needs.load-testing.result }}',
              'Bundle Analysis': '${{ needs.bundle-analysis.result }}'
            };
            
            let summary = '## ğŸš€ Performance Test Results\n\n';
            summary += '| Test Type | Status | Details |\n';
            summary += '|-----------|--------|----------|\n';
            
            for (const [test, status] of Object.entries(results)) {
              const emoji = status === 'success' ? 'âœ…' : 
                           status === 'failure' ? 'âŒ' : 
                           status === 'skipped' ? 'â­ï¸' : 'â³';
              
              let details = '';
              if (status === 'success') {
                details = 'All metrics within budget';
              } else if (status === 'failure') {
                details = 'Some metrics exceeded budget';
              } else if (status === 'skipped') {
                details = 'Test skipped for this run';
              }
              
              summary += `| ${test} | ${emoji} ${status} | ${details} |\n`;
            }
            
            summary += '\n### ğŸ“‹ Performance Metrics\n\n';
            summary += '- **Lighthouse Score**: Target â‰¥85\n';
            summary += '- **Response Time**: Target <2s average\n';
            summary += '- **P95 Response**: Target <5s\n';
            summary += '- **Bundle Size**: Target <1MB total\n';
            summary += '- **Error Rate**: Target <5%\n\n';
            
            if (Object.values(results).includes('failure')) {
              summary += 'âš ï¸ **Performance issues detected.** Check detailed reports in artifacts.\n\n';
            } else {
              summary += 'ğŸ‰ **All performance tests passed!**\n\n';
            }
            
            summary += '_Performance tests help ensure LocalLoop remains fast and responsive._';
            
            console.log(summary); 